<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fine Grained Classification on Hong-Yu&#39;s Homepage</title>
    <link>https://funnyzhou.github.io/tags/fine-grained-classification/index.xml</link>
    <description>Recent content in Fine Grained Classification on Hong-Yu&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Your Name</copyright>
    <atom:link href="/tags/fine-grained-classification/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</title>
      <link>https://funnyzhou.github.io/project/sos/</link>
      <pubDate>Mon, 21 Aug 2017 18:54:21 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/project/sos/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/SoSNet.png&#34; alt=&#34;image0&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;font size=5&gt;The difficulty of image recognition has gradually increased from general category recognition to fine-grained recognition and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on the classification between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. &lt;font color=blue&gt;&lt;strong&gt;Sunrise vs. sunset is a difficult recognition task, which is challenging even for humans. Towards understanding this new problem, we first collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images.&lt;/strong&gt;&lt;/font&gt; Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans. We also apply our approach to existing subtle attribute recognition problems, such as temperature estimation, and achieve state-of-the-art results.&lt;/font&gt;&lt;/p&gt;

&lt;h1 id=&#34;subtle-attribute-recognition&#34;&gt;Subtle Attribute Recognition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/comparison.png&#34; alt=&#34;image1&#34; /&gt;
&lt;font size=5&gt;A comparison between subtle attributes and transient attributes is shown in the &lt;strong&gt;figure above&lt;/strong&gt;. We can see that compared with transient attribute, subtle attribute can be more detailed.&lt;/font&gt;&lt;/p&gt;

&lt;h2 id=&#34;sos-a-new-benchmark&#34;&gt;SoS: A New Benchmark&lt;/h2&gt;

&lt;p&gt;&lt;font size=5&gt;The dataset contains more 10k images from over 100 webcams. We randomly choose some images from SoS dataset which are displayed below.
&lt;img src=&#34;https://funnyzhou.github.io/post/img/sos.png&#34; alt=&#34;image2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;human-performance&#34;&gt;Human performance&lt;/h2&gt;

&lt;p&gt;For a better study, we also construct two types of tasks: an &lt;strong&gt;easy&lt;/strong&gt; task and a &lt;strong&gt;hard&lt;/strong&gt; one. The performance of VGG models and human beings are also reported in Table 1.&lt;/font&gt;
&lt;img src=&#34;https://funnyzhou.github.io/post/img/human.png&#34; alt=&#34;image3&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h1&gt;

&lt;h2 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/pair_constrains.png&#34; alt=&#34;image4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sosnet-performance&#34;&gt;SoSNet Performance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/experiments.png&#34; alt=&#34;image5&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;publication&#34;&gt;Publication&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  BMVC 2017ã€€&amp;nbsp; &lt;a href=&#34;https://arxiv.org/pdf/1707.06335.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/1707.06335&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://mega.nz/#!s8IhgJCC!nSDluCHXNQddrb7Sh_JG3AvIeP2XwhAU66_im_7FByY&#34; target=&#34;_blank&#34;&gt;Dataset&lt;/a&gt; | Slide | Video | Poster&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
