<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hong-Yu&#39;s Homepage on Hong-Yu&#39;s Homepage</title>
    <link>https://funnyzhou.github.io/index.xml</link>
    <description>Recent content in Hong-Yu&#39;s Homepage on Hong-Yu&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Your Name</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</title>
      <link>https://funnyzhou.github.io/project/sos/</link>
      <pubDate>Mon, 21 Aug 2017 18:54:21 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/project/sos/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/SoSNet.png&#34; alt=&#34;image0&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;font size=5&gt;The difficulty of image recognition has gradually increased from general category recognition to fine-grained recognition and to the recognition of some subtle attributes such as temperature and geolocation. In this paper, we try to focus on the classification between sunrise and sunset and hope to give a hint about how to tell the difference in subtle attributes. &lt;font color=blue&gt;&lt;strong&gt;Sunrise vs. sunset is a difficult recognition task, which is challenging even for humans. Towards understanding this new problem, we first collect a new dataset made up of over one hundred webcams from different places. Since existing algorithmic methods have poor accuracy, we propose a new pairwise learning strategy to learn features from selective pairs of images.&lt;/strong&gt;&lt;/font&gt; Experiments show that our approach surpasses baseline methods by a large margin and achieves better results even compared with humans. We also apply our approach to existing subtle attribute recognition problems, such as temperature estimation, and achieve state-of-the-art results.&lt;/font&gt;&lt;/p&gt;

&lt;h1 id=&#34;subtle-attribute-recognition&#34;&gt;Subtle Attribute Recognition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/comparison.png&#34; alt=&#34;image1&#34; /&gt;
&lt;font size=5&gt;A comparison between subtle attributes and transient attributes is shown in the &lt;strong&gt;figure above&lt;/strong&gt;. We can see that compared with transient attribute, subtle attribute can be more detailed.&lt;/font&gt;&lt;/p&gt;

&lt;h2 id=&#34;sos-a-new-benchmark&#34;&gt;SoS: A New Benchmark&lt;/h2&gt;

&lt;p&gt;&lt;font size=5&gt;The dataset contains more 10k images from over 100 webcams. We randomly choose some images from SoS dataset which are displayed below.
&lt;img src=&#34;https://funnyzhou.github.io/post/img/sos.png&#34; alt=&#34;image2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;human-performance&#34;&gt;Human performance&lt;/h2&gt;

&lt;p&gt;For a better study, we also construct two types of tasks: an &lt;strong&gt;easy&lt;/strong&gt; task and a &lt;strong&gt;hard&lt;/strong&gt; one. The performance of VGG models and human beings are also reported in Table 1.&lt;/font&gt;
&lt;img src=&#34;https://funnyzhou.github.io/post/img/human.png&#34; alt=&#34;image3&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h1&gt;

&lt;h2 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/pair_constrains.png&#34; alt=&#34;image4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sosnet-performance&#34;&gt;SoSNet Performance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/experiments.png&#34; alt=&#34;image5&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;publication&#34;&gt;Publication&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  BMVC 2017　&amp;nbsp; &lt;a href=&#34;https://arxiv.org/pdf/1707.06335.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/1707.06335&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://mega.nz/#!s8IhgJCC!nSDluCHXNQddrb7Sh_JG3AvIeP2XwhAU66_im_7FByY&#34; target=&#34;_blank&#34;&gt;Dataset&lt;/a&gt; | Slide | Video | Poster&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</title>
      <link>https://funnyzhou.github.io/project/adaptive_feeding/</link>
      <pubDate>Sat, 29 Jul 2017 22:31:54 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/project/adaptive_feeding/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/arch_af.png&#34; alt=&#34;image0&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;font size=5&gt;
Object detection aims at high speed and accuracy simultaneously. However, fast models are usually less accurate, while accurate models cannot satisfy our need for speed. A fast model can be 10 times faster but 50% less accurate than an accurate model. &lt;font color=blue&gt;&lt;strong&gt;In this paper, we propose Adaptive Feeding (AF) to combine a fast (but less accurate) detector and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an appropriate detector for it.&lt;/strong&gt;&lt;/font&gt; In practice, we build a cascade of detectors, including the AF classifier which make the easy vs. hard decision and the two detectors. The AF classifier can be tuned to obtain different tradeoff between speed and accuracy, which has negligible training time and requires no additional training data. Experimental results on the PASCAL VOC, MS COCO and Caltech Pedestrian datasets confirm that AF has the ability to achieve comparable speed as the fast detector and comparable accuracy as the accurate one at the same time. As an example, by combining the fast SSD300 with the accurate SSD500 detector, AF leads to 50% speedup over SSD500 with the same precision on the VOC2007 test set.&lt;/p&gt;

&lt;h1 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h1&gt;

&lt;p&gt;In following table, the &lt;strong&gt;Basic&lt;/strong&gt; model refers to a fast detector while &lt;strong&gt;Partner&lt;/strong&gt; represents the accurate model. $P_2$ and $P_1$ are $mAPI$ (for its definition, please refer to the original paper) of Partner and Basic detectors, respectively.
&lt;figure class=&#34;half&#34;&gt;
    &lt;img src=&#34;https://funnyzhou.github.io/post/img/table2.png&#34; width=&#34;900&#34;&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h1&gt;

&lt;h2 id=&#34;pascal-voc-2007&#34;&gt;Pascal VOC 2007&lt;/h2&gt;

&lt;figure class=&#34;half&#34;&gt;
    &lt;img src=&#34;https://funnyzhou.github.io/post/img/table4.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;pascal-voc-2012&#34;&gt;Pascal VOC 2012&lt;/h2&gt;

&lt;figure class=&#34;half&#34;&gt;
    &lt;img src=&#34;https://funnyzhou.github.io/post/img/table5.png&#34; width=&#34;1500&#34;&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;publication&#34;&gt;Publication&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ICCV 2017　&amp;nbsp; &lt;a href=&#34;https://arxiv.org/pdf/1707.06399.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/1707.06399&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt; | Slide | Video | Poster&lt;/p&gt;

&lt;h1 id=&#34;downloads&#34;&gt;Downloads&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Code [coming soon]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;This work was supported in part by the National Natural Science Foundation of China under Grant No. 61422203.&lt;/font&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised CNN</title>
      <link>https://funnyzhou.github.io/project/weakly_supervised_cnn/</link>
      <pubDate>Sat, 29 Jul 2017 22:26:47 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/project/weakly_supervised_cnn/</guid>
      <description>&lt;p&gt;This is an implementation of the method in &lt;strong&gt;Is object localization for free? – Weakly-supervised learning with convolutional neural networks&lt;/strong&gt; using MatConvNet. For more details, please refer to &lt;a href=&#34;https://github.com/funnyzhou/weakcnn&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/weakcnn.png&#34; alt=&#34;image1&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CAM in MatConvNet</title>
      <link>https://funnyzhou.github.io/project/cam_matconvnet/</link>
      <pubDate>Sat, 29 Jul 2017 22:19:54 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/project/cam_matconvnet/</guid>
      <description>&lt;p&gt;This is an implementation of the method in &lt;strong&gt;Learning Deep Features for Discriminative Localization&lt;/strong&gt; using MatConvNet. For more details, please refer to &lt;a href=&#34;https://github.com/funnyzhou/CAM_MatConvNet&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://funnyzhou.github.io/post/img/ex1.png&#34; alt=&#34;image1&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</title>
      <link>https://funnyzhou.github.io/publication/adaptive-feeding/</link>
      <pubDate>Sat, 29 Jul 2017 22:08:21 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/publication/adaptive-feeding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</title>
      <link>https://funnyzhou.github.io/publication/sos/</link>
      <pubDate>Sat, 29 Jul 2017 21:59:30 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/publication/sos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Content-Based Image Recovery</title>
      <link>https://funnyzhou.github.io/publication/content-recovery/</link>
      <pubDate>Sat, 29 Jul 2017 21:59:27 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/publication/content-recovery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Awesome object detection</title>
      <link>https://funnyzhou.github.io/post/awesome-object-detection/</link>
      <pubDate>Thu, 30 Mar 2017 16:16:26 +0800</pubDate>
      
      <guid>https://funnyzhou.github.io/post/awesome-object-detection/</guid>
      <description>

&lt;p&gt;A curated list of resources I have read or used. Besides, each material is attached with a short piece of abstract.&lt;/p&gt;

&lt;h1 id=&#34;updated-30-03-2017&#34;&gt;(updated 30/03/2017)&lt;/h1&gt;

&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#papers&#34;&gt;Papers&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-cnn&#34;&gt;R-CNN Series&lt;/a&gt; (&lt;a href=&#34;#sec1_2017&#34;&gt;2017&lt;/a&gt; &lt;a href=&#34;#sec1_2016&#34;&gt;2016&lt;/a&gt; &lt;a href=&#34;#sec1_2015&#34;&gt;2015&lt;/a&gt; &lt;a href=&#34;#sec1_2014&#34;&gt;2014&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ssd&#34;&gt;Single Shot Detectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding&#34;&gt;Towards A Better Understanding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;

&lt;h3 id=&#34;r-cnn-series&#34;&gt;R-CNN Series&lt;/h3&gt;

&lt;h4 id=&#34;2015&#34;&gt;2015&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Faster R-CNN&lt;a href=&#34;https://github.com/ShaoqingRen/faster_rcnn&#34; target=&#34;_blank&#34;&gt;[Code_Matlab]&lt;/a&gt;&lt;a href=&#34;https://github.com/rbgirshick/py-faster-rcnn&#34; target=&#34;_blank&#34;&gt;[Code_Python]&lt;/a&gt;&lt;a href=&#34;https://github.com/CharlesShang/TFFRCNN&#34; target=&#34;_blank&#34;&gt;[Code_Tensorflow]&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34; target=&#34;_blank&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;, TPAMI, 2016. (An older version appeared in NIPS 2015)&lt;/li&gt;
&lt;li&gt;Summary: &lt;strong&gt;A Region Proposal Network (RPN).&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Code note: The python version is mostly used.
&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Fast R-CNN&lt;a href=&#34;https://github.com/rbgirshick/fast-rcnn&#34; target=&#34;_blank&#34;&gt;[Code_Python]&lt;/a&gt;&lt;a href=&#34;https://github.com/ShaoqingRen/faster_rcnn&#34; target=&#34;_blank&#34;&gt;[Code_Matlab]&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Ross Girshick. &lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34;&gt;Fast R-CNN&lt;/a&gt;, ICCV, 2015.&lt;/li&gt;
&lt;li&gt;Summary: &lt;strong&gt;The basic network architecture for later R-CNN based researches.&lt;/strong&gt;
&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Multibox_v2 &lt;a href=&#34;https://github.com/google/multibox&#34; target=&#34;_blank&#34;&gt;[Code]&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe. &lt;a href=&#34;https://arxiv.org/pdf/1412.1441.pdf&#34; target=&#34;_blank&#34;&gt;Scalable High Quality Object Detection&lt;/a&gt;, CVPR, 2015.&lt;/li&gt;
&lt;li&gt;Summary: &lt;strong&gt;The basic idea is similar to RPN&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2014&#34;&gt;2014&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;R-CNN &lt;a href=&#34;https://github.com/rbgirshick/rcnn&#34; target=&#34;_blank&#34;&gt;[Code]&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. &lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34; target=&#34;_blank&#34;&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;, CVPR, 2014.&lt;/li&gt;
&lt;li&gt;Summary: &lt;strong&gt;The first time object detection combined with deep learning.&lt;/strong&gt;
&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Multibox_v1 &lt;a href=&#34;https://github.com/google/multibox&#34; target=&#34;_blank&#34;&gt;[Code]&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov. &lt;a href=&#34;https://arxiv.org/pdf/1312.2249.pdf&#34; target=&#34;_blank&#34;&gt;Scalable Object Detection using Deep Neural Networks&lt;/a&gt;, CVPR, 2014.&lt;/li&gt;
&lt;li&gt;Summary: &lt;strong&gt;This paper tought us that we can generate proposals by using a deep neural network.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://funnyzhou.github.io/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://funnyzhou.github.io/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://gcushen.github.io/hugo-academic-demo/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
