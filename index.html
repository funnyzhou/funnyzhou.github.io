<!-- saved from url=(0031)https://funnyzhou.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hong-Yu Zhou</title>

<link rel="stylesheet" href="./zhouhy_files/bootstrap.min.css">

<script src="./zhouhy_files/jquery-3.1.1.slim.min.js"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");

  body
  {
  	font-family: 'Source Sans Pro', sans-serif;
      background-color : #CDCDCD;
      font-size: 19px;
  }
      .content
  	{
      		width : 900px;
      		padding : 25px 30px;
      		margin : 25px auto;
      		background-color : #fff;
      		box-shadow: 0px 0px 10px #999;
      		border-radius: 15px; 
  	}	
  	table
  	{
  		padding: 5px;
  	}
  	
  	table.pub_table,td.pub_td1,td.pub_td2
  	{
  		padding: 8px;
  		width: 850px;
          border-collapse: separate;
          border-spacing: 15px;
          margin-top: -5px;
  	}

  	td.pub_td1
  	{
  		width:50px;
  	}
      td.pub_td1 img
      {
          height:120px;
          width: 160px;
      }
  	
  	div#container
  	{
  		margin-left: auto;
  		margin-right: auto;
  		width: 820px;
  		text-align: left;
  		position: relative;
  		background-color: #FFF;
  	}
  	div#DocInfo
  	{
  		color: #1367a7;
  		height: 158px;
  	}
  	h4,h3,h2,h1
  	{
  		color: #3B3B3B;
  	}
  	h2
  	{
  		font-size:130%;
  	}
  	p
  	{
  		color: #5B5B5B;
  		margin-bottom: 50px;
  	}
  	p.caption
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 600px;
  	}
  	p.caption2
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 800px;
  	}
  	#header_img
  	{
  		position: absolute;
  		top: 0px; right: 0px;
      }
  	a:link,a:visited
  	{
  		color: #1367a7;
  		text-decoration: none;
  	}

      #hku_logo {
          position: absolute;
          left: 646px;
          top: 14px;
          width: 200px;
          height: 20px;
      }
     
      table.pub_table tr {
          outline: thin dotted #666666;
      }
      .papericon {
          border-radius: 8px; 
          -moz-box-shadow: 3px 3px 6px #888;
          -webkit-box-shadow: 3px 3px 6px #888;
          box-shadow: 3px 3px 6px #888;
      }
      .one{
          margin-bottom: 1em;
      }
      .two{
          margin-top: 1em;
      }
      .media {
          margin-bottom: 15px;
          margin-left: 10px;
      }

      .media-body {
          margin-top: 5px;
          padding-left: 20px;
      }

      .publication {
          margin-bottom: 15px;
      }

      .papers-selected .publication {
          display: none;
      }

      .papers-selected .book-chapters {
          display: none;
      }

      .papers-selected #show-selected {
          color: black;
          text-decoration: underline;
      }

      .papers-selected .paperhi {
          display: flex;
      }

      .papers-selected .paper-year {
          display: none;
      }

      .papers-by-date #show-by-date {
          color: black;
          text-decoration: underline;
      }

      .papers-by-date .paper-selected {
          display: none;
      }

      .papers-by-date .book-chapters {
          display: none;
      }

      .book-chapters #book-chapters {
          color: black;
          text-decoration: underline;
      }

      .book-chapters .paper-selected,
      .book-chapters .paper-year,
      .book-chapters .publication {
          display: none;
      }

      .book-chapters .chapter {
          display: flex;
      }

      /* .papers-by-date .paperhi {
          display: none;
      } */

      .hidden>div {
          display: none;
      }

      .visible>div {
          display: block;
      }
</style>

<!-- <script type="text/javascript" async="" src="./zhouhy_files/ga.js"></script><script async="" src="./zhouhy_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23931362-2', 'nju.edu.cn');
  ga('send', 'pageview');

</script> -->
<!-- <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23931362-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

    var myPix = new Array("zhouhy_files/me_venice.png")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };

</script> -->
<script>
  $(document).ready(function () {
            $('#show-selected').click(function () {
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').addClass('papers-selected');
            });

            $('#show-by-date').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').addClass('papers-by-date');
            });

            $('#book-chapters').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('papers-by-date');
            });

            $('.papers-container').addClass('papers-selected');


            $('.text_container').addClass("hidden");
            $('.text_container').click(function () {
                var $this = $(this);

                if ($this.hasClass("hidden")) {
                    $(this).removeClass("hidden").addClass("visible");

                } else {
                    $(this).removeClass("visible").addClass("hidden");
                }
            });

  });
</script>
</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="./zhouhy_files/me_venice.png" style="float:left; padding-right:10px" height="200px"></td>
	<td>
	<div id="DocInfo">
		<h1>Hong-Yu Zhou</h1>
        <font size="4.5"> PhD Candidate</font><br>
        <font size="4.5"> Department of Computer Science</font><br>
        <font size="4.5"> Advisor: Yizhou Yu</font><br>
        <font size="4.5">Office: Rm 335A, Haking Wong Building, HKU</font><br> 
        <font size="4.5">Email Address:</font><br> 
        <a href="mailto:whuzhouhongyu@gmail.com"> <font size="4">whuzhouhongyu at gmail dot com </font></a><br>
	</div><br>
        <div id="hku_logo">
        <a href="https://www.hku.hk/"><img src="./zhouhy_files/hku_icon.jpg" height="170px" class="papericon"></a>
    </div>
	</td>
	</tr>
	</tbody></table>

	<h2>Bio</h2>
    <ul>
        <li>Now, I am a second-year PhD candidate at <a href="https://www.cs.hku.hk/">Department of Computer Science</a>, <a href="">The University of Hong Kong</a>. My advisor is Prof. <a href="https://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>. My research interest is medical image analysis, where my main focus is on how to learn well-generalized representations for medical images.</li>
        <li>From Jul. 2018 to Jul. 2020, I was a senior researcher at Tencent focusing on Medical x AI applications. My group leader is Dr. <a href="https://sites.google.com/site/yefengzheng/">Yefeng Zheng</a>.</li>
        <li>I got my master degree from <a href="http://www.lamda.nju.edu.cn/MainPage.ashx">LAMDA Group</a> at <a href="https://www.nju.edu.cn/en/main.psp">Nanjing University</a> in 2018, advised by Prof. <a href="https://cs.nju.edu.cn/wujx/">Jianxin Wu</a>. Before that, I received my bachelor degree from <a href="https://en.whu.edu.cn/">Wuhan University</a>.</li>
    </ul>
	
<!--     <h2>Recent News</h2>
    <ul>
      <li>[2020/07] Three papers have been accepted to MICCAI 2020.</li>
      <li>[2020/02] Our team has won 4 championships in 4 different global competitions in 2019!</li>
      <li>[2019/12] Second place in ODIR 2019, held by Peking University.</li>
      <li>[2019/11] First places in all three tracks in 2019 MICCAI AGE Challenge.</li>
      <li>[2019/10] We won the <b>championship</b> in AASCE 2019, held by MICCAI 2019. The proposed algorithm is able to estimate the spinal curvature in Adolescent Idiopathic Scoliosis (AIS) accurately. Press coverage: <a href="https://tech.china.com/article/20190929/kejiyuan0129377260.html">中华网</a> <a href="http://www.kejixun.com/article/190929/474694.shtml">科技讯</a> <a href=" http://yisheng.12120.net/news/jkpt/content_0910114529.html">中国好医生网</a> <a href="https://tech.ifeng.com/c/7qMGUlwISFH">凤凰网</a>.</li>
      <li>[2019/06] We won the <b>first place</b> in Segthor 2019 Challenge, held by ISBI 2019. The proposed approach aims at segmenting organs at risk segmentation in Computed Tomography (CT) images. We achieved the highest Dice scores on Esophagus, Heart and Trachea. Press coverage: <a href="https://tech.qq.com/a/20190619/003614.htm">腾讯科技</a> <a href="https://www.jiqizhixin.com/articles/2019-06-19-8">机器之心</a> <a href="https://tech.huanqiu.com/article/9CaKrnKl1aA">环球网</a> <a href="https://www.qbitai.com/2019/06/3231.html">量子位</a> <a href="https://www.sohu.com/a/322090752_610300">搜狐网</a>.</li>
      <li>[2019/05] Our team won the <b>first place</b> in AI4Health Challeng, held by Thales group which provides solutions for over 50% of worldwide radiology examinations. In this challenge, we proposed a novel methodology to detect key points in X-rays very accurately (the mean error is smaller than <b>0.3 pixel</b>). Press coverage: <a href="http://www.ce.cn/xwzx/gnsz/gdxw/201907/12/t20190712_32605039.shtml">中国经济网</a> <a href="https://tech.huanqiu.com/article/9CaKrnKluBC">环球网</a> <a href="https://cloud.tencent.com/developer/article/1600187">腾讯新闻</a> <a href=""> <a href="http://www.geekpark.net/news/244645">极客公园</a> <a href="http://news.eastday.com/eastday/13news/auto/news/china/20190712/u7ai8688252.html">东方网</a>.</li>
    </ul> -->
      <div class="papers-container">
          <div class="publication-anchor" id="publication-anchor"></div>
          <h2 class="paperlo">Paper <span id="paper-show">(<span id="show-selected">Selected List (Highlight)</span> /
                  <span id="show-by-date">Full List</span>)</span>
          </h2>
           <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2022</p></h3>
          <div class="paper-2022">  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification</b></a>
                            <br> Zhixiong Yang*, Junwen Pan*, Yanzhan Yang, Xiaozhou Shi, <b>Hong-Yu Zhou</b>, Zhicheng Zhang, Cheng Bian
                            <br>[<a href="https://arxiv.org/abs/2206.02355">Paper</a>, <a href="https://github.com/skyz215/ProCo" target="_blank">Code</a>]
                            <br><i>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> Relation Matters: Foreground-aware Graph-based Relational Reasoning for Domain Adaptive Object Detection</b></a>
                            <br> Chaoqi Chen, Jiongcheng Li, <b>Hong-Yu Zhou</b>, Xiaoguang Han, Yue Huang, Xinghao Ding, Yizhou Yu
                            <br>[<a href="https://arxiv.org/abs/2206.02355">Paper</a>, <a href="" target="_blank">Code</a>]
                            <br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="">
                            <b> GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Liansheng Wang, Yizhou Yu
                            <br>[<a href="">Paper</a>, <a href="" target="_blank">Code</a>]
                            <br><i>IEEE Transactions on Medical Imaging (TMI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522001530">
                            <b> MASS: Modality-collaborative semi-supervised segmentation by exploiting cross-modal consistency from unpaired CT and MRI images</b></a>
                            <br> Xiaoyu Chen*, <b>Hong-Yu Zhou</b>*, FengLiu, Jiansen Guo, Liansheng Wang, YizhouYu
                            <br>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522001530">Paper</a>, <a href="https://github.com/xy123chen/MASS" target="_blank">Code</a>]
                            <br><i>Medical Image Analysis, 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2203.09064">
                            <b> Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning</b></a>
                            <br> Yangji He, Weihan Liang, Dongyang Zhao, <b>Hong-Yu Zhou</b>, Weifeng Ge, Yizhou Yu, and Wenqiang Zhang
                            <br>[<a href="https://arxiv.org/pdf/2203.09064.pdf">Paper</a>, <a href="https://github.com/StomachCold/HCTransformers" target="_blank">Code</a>]
                            <br><i>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2201.01426">
                            <b> Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training</b></a>
                            <br> Shu Zhang, Zihao Li, <b>Hong-Yu Zhou</b>, Jiechao Ma, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2201.01426" target="_blank">Paper</a>, <a href="https://github.com/urmagicsmine/CSPR" target="_blank">Code</a>]
                            <br><i>Technical Report.</i>
                            <br>
                        </div>
                    </div>
          	        <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2111.03452">
                            <b> Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Xiaoyu Chen*, Yinghao Zhang*, Ruibang Luo, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://www.nature.com/articles/s42256-021-00425-9">Nat. Mach. Intel.</a>, <a href="https://arxiv.org/abs/2111.03452" target="_blank">Arxiv</a>, <a href="https://github.com/funnyzhou/REFERS" target="_blank">Code</a>]
                            <br><i>Nature Machine Intelligence, 2022.</i>
                            <br>
                        </div>
                    </div>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2021</p></h3>
          <div class="paper-2021">    
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Sibei Yang, Xiaoguang Han, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.04379" target="_blank">Paper</a>, <a href="https://github.com/Luchixiang/PCRL" target="_blank">Code</a>]
                            <br><i> International Conference on Computer Vision (ICCV), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> nnFormer: Interleaved Transformer for Volumetric Segmentation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Jiansen Guo*, Yinghao Zhang*, Lequan Yu, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.03201" target="_blank">Paper</a>, <a href="https://github.com/282857341/nnFormer" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2108.05305">
                            <b> ConvNets vs. Transformers: Whose Visual Representations are More Transferable?</b></a>
                            <br> <b>Hong-Yu Zhou</b>, Chixiang Lu, Sibei Yang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2108.05305" target="_blank">Paper</a>]
                            <br><i> ICCV workshop on Multi-Task Learning in Computer Vision (DeepMTL). </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.01755">
                            <b> Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition</b></a>
                            <br> Bin-Bin Gao, <b>Hong-Yu Zhou</b>.
                            <br>[<a href="https://arxiv.org/abs/2007.01755" target="_blank">Paper</a>, <a href="https://github.com/gaobb/MCAR" target="_blank">Code</a>]
                            <br><i> IEEE Transactions on Image Processing (TIP), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2103.16344">
                            <b> Generalized Organ Segmentation by Imitating One-Shot Reasoning Using Anatomical Correlation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Hualuo Liu*, Shilei Cao*, Dong Wei, Chixiang Lu, Yizhou Yu, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2103.16344" target="_blank">Paper</a>]
                            <br><i> International Conference on Information Processing in Medical Imaging (IPMI), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.medrxiv.org/content/10.1101/2021.06.02.21258256v1">
                            <b> SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency and Heterogeneous Perturbation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chengdi Wang*, Haofeng Li*, Gang Wang, Shu Zhang, Weimin Li, Yizhou Yu.
                            <br>[<a href="https://www.medrxiv.org/content/10.1101/2021.06.02.21258256v1" target="_blank">Paper</a>]
                            <br><i> Medical Image Analysis, 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.sciencedirect.com/science/article/pii/S1361841521001614">
                            <b> Evaluation and Comparison of Accurate Automated Spinal Curvature Estimation Algorithms with Spinal Anterior-posterior X-Ray Images: The AASCE2019 Challenge</b></a>
                            <br> Liansheng Wang, Cong Xie, Yi Lin, <b>Hong-Yu Zhou</b>, Kailin Chen, Dalong Cheng, Florian Dubost, Benjamin Collery, Bidur Khanal, Bishesh Khanal, Rong Tao, Shangliang Xu, Upasana Upadhyay Bharadwaj, Zhusi Zhong, Jie Li, Shuxin Wang, Shuo Li.
                            <br>[<a href="https://www.sciencedirect.com/science/article/pii/S1361841521001614" target="_blank">Paper</a>]
                            <br><i> Medical Image Analysis, 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2107.08715">
                            <b> Recist-Net: Lesion Detection via Grouping Keypoints on Recist-Based Annotation</b></a>
                            <br> Cong Xie, Shilei Cao, Dong Wei, <b>Hongyu Zhou</b>, Kai Ma, Xianli Zhang, Buyue Qian, Liansheng Wang, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/pdf/2107.08715.pdf" target="_blank">Paper</a>]
                            <br><i> IEEE 18th International Symposium on Biomedical Imaging (ISBI), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2102.13280">
                            <b> MixSearch: Searching for Domain Generalized Medical Image Segmentation Architectures</b></a>
                            <br> Luyan Liu*<sup>,&#8224;</sup>, Zhiwei Wen*, Songwei Liu*, <b>Hong-Yu Zhou</b>*<sup>,&#8224;</sup>, Hongwei Zhu, Weicheng Xie, Linlin Shen, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2102.13280" target="_blank">Paper</a>, <a href="https://github.com/lswzjuer/NAS-WDAN/" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf">
                            <b> Bottom-Up Shift and Reasoning for Referring Image Segmentation</b></a>
                            <br> Sibei Yang, Meng Xia, Guanbin Li, <b>Hong-Yu Zhou</b>, Yizhou Yu.
                            <br>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf" target="_blank">Paper</a>, <a href="https://github.com/incredibleXM/BUSNet" target="_blank">Code</a>]
                            <br><i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. </i>
                            <br>
                        </div>
                    </div>       
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2020</p></h3>
          <div class="paper-2020">
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.10007">
                            <b> A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation</b></a>
                            <br> Munan Ning, Cheng Bian, Donghuan Lu, <b>Hong-Yu Zhou</b>, Shuang Yu, Chenglang Yuan, Yang Guo, Yaohua Wang, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.10007" target="_blank">Paper</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>   
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.14848">
                            <b> Difficulty-aware Glaucoma Classification with Multi-rater Consensus Modeling </b></a>
                            <br> Shuang Yu*, <b>Hong-Yu Zhou</b>*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.14848" target="_blank">Paper</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.07423">
                            <b> Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations </b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Shuang Yu*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.07423" target="_blank">Paper</a>, <a href="https://github.com/funnyzhou/C2L_MICCAI2020" target="_blank">Code</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>          
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://web.archive.org/web/20210429012105id_/https://ieeexplore.ieee.org/ielx7/6221020/9216186/09172136.pdf">
                            <b> Efficient and Effective Training of Covid-19 Classification Networks with Self-supervised Dual-track Learning to Rank </b></a>
                            <br> Yuexiang Li, Dong Wei, Jiawei Chen, Shilei Cao, <b>Hongyu Zhou</b>, Yanchun Zhu, Jianrong Wu, Lan Lan, Wenbo Sun, Tianyi Qian, Kai Ma, Haibo Xu, Yefeng Zheng.
                            <br>[<a href="https://web.archive.org/web/20210429012105id_/https://ieeexplore.ieee.org/ielx7/6221020/9216186/09172136.pdf" target="_blank">Paper</a>]
                            <br><i> IEEE Journal of Biomedical and Health Informatics (JBHI), 2020. </i>
                            <br>
                        </div>
                    </div>         
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.01771">
                            <b> Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation </b></a>
                            <br> Bin-Bin Gao, Xin-Xin Liu, <b>Hong-Yu Zhou</b>, Jianxin Wu, Xin Geng.
                            <br>[<a href="https://arxiv.org/abs/2007.01771" target="_blank">Paper</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2019 and Before</p></h3>
          <div class="paper-2019">
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://link.springer.com/chapter/10.1007/978-3-030-39752-4_7">
                            <b> Seg4Reg Networks for Automated Spinal Curvature Estimation </b></a>
                            <br> Yi Lin, <b>Hong-Yu Zhou</b><sup>&#8224;</sup>, Kai Ma, Xin Yang, Yefeng Zheng.
                            <br>[<a href="https://link.springer.com/chapter/10.1007/978-3-030-39752-4_7" target="_blank">Paper</a>]
                            <br><font color="red">Ranked 1st place in 2019 MICCAI AASCE Challenge.</font>
                            <br><i> Computational Methods and Clinical Applications for Spine Imaging, 2019. </i>
                            <br>
                        </div>
                    </div>    
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://cs.nju.edu.cn/wujx/paper/FCS2018_CodeAttention.pdf">
                            <b> CodeAttention: Translating Source Code to Comments by Exploiting the Code Constructs </b></a>
                            <br> Wenhao Zheng, <b>Hongyu Zhou</b>, Ming Li, Jianxin Wu.
                            <br>[<a href="https://cs.nju.edu.cn/wujx/paper/FCS2018_CodeAttention.pdf" target="_blank">Paper</a>]
                            <br><i> Frontiers of Computer Science, 2019. </i>
                            <br>
                        </div>
                    </div>         
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1812.05313">
                            <b> When Semi-supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Avital Oliver, Jianxin Wu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/1812.05313" target="_blank">Paper</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>       
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://ieeexplore.ieee.org/abstract/document/8416559">
                            <b> ThiNet: Pruning CNN Filters for a Thinner Net </b></a>
                            <br> Jian-Hao Luo, Hao Zhang, <b>Hong-Yu Zhou</b>, Chen-Wei Xie, Jianxin Wu, Weiyao Lin.
                            <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8416559" target="_blank">Paper</a>, <a href="https://github.com/Roll920/ThiNet_Code" target="_blank">Code</a>]
                            <br><i> IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI), 2018. </i>
                            <br>
                        </div>
                    </div>   
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="http://palm.seu.edu.cn/xgeng/files/ijcai18d.pdf">
                            <b> Age Estimation Using Expectation of Label Distribution Learning </b></a>
                            <br> Bin-Bin Gao, <b>Hong-Yu Zhou</b>, Jianxin Wu, Xin Geng.
                            <br>[<a href="http://palm.seu.edu.cn/xgeng/files/ijcai18d.pdf" target="_blank">Paper</a>, <a href="https://github.com/gaobb/DLDL-v2" target="_blank">Code</a>]
                            <br><i> International Joint Conference on Artificial Intelligence (IJCAI), 2018. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1804.06242">
                            <b> Vortex Pooling: Improving Context Representation in Semantic Segmentation </b></a>
                            <br> Chen-Wei Xie, <b>Hong-Yu Zhou</b>, Jianxin Wu.
                            <br>[<a href="https://arxiv.org/abs/1804.06242" target="_blank">Paper</a>,  <a href="https://github.com/MTCloudVision/deeplabv3-mxnet_gluon" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1707.06335">
                            <b> Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Bin-Bin Gao, Jianxin Wu.
                            <br>[<a href="https://arxiv.org/abs/1707.06335" target="_blank">Paper</a>]
                            <br><i> The British Machine Vision Association (BMVC), 2017. </i>
                            <br>
                        </div>
                    </div>  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper.pdf">
                            <b> Adaptive feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Bin-Bin Gao, Jianxin Wu.
                            <br>[<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper.pdf" target="_blank">Paper</a>,  <a href="https://github.com/funnyzhou/Adaptive_Feeding" target="_blank">Code</a>]
                            <br><i> International Conference on Computer Vision (ICCV), 2017. </i>
                            <br>
                        </div>
                    </div>  
          <div class="paper-selected">
          <br>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2111.03452">
                            <b> Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Xiaoyu Chen*, Yinghao Zhang*, Ruibang Luo, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://www.nature.com/articles/s42256-021-00425-9">Nat. Mach. Intel.</a>, <a href="https://arxiv.org/abs/2111.03452" target="_blank">Arxiv</a>, <a href="https://github.com/funnyzhou/REFERS" target="_blank">Code</a>]
                            <br><font color="red">We propose cross-supervised learning to learn X-ray representations from associated medical reports, which achieves great improvements over previous self-supervised methodologies.</font>
                            <br><i> Nature Machine Intelligence, 2022</i>
                            <br>
                        </div>
                    </div>
          
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Sibei Yang, Xiaoguang Han, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.04379" target="_blank">Paper</a>, <a href="https://github.com/Luchixiang/PCRL" target="_blank">Code</a>]
                            <br><font color="red">New state-of-the-arts of self-supervised medical image pre-training on both 2D and 3D data.</font>
                            <br><i> International Conference on Computer Vision (ICCV), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> nnFormer: Interleaved Transformer for Volumetric Segmentation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Jiansen Guo*, Yinghao Zhang*, Lequan Yu, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.03201" target="_blank">Paper</a>, <a href="https://github.com/282857341/nnFormer" target="_blank">Code</a>]
                            <br><font color="red">A transformer-based segmentation network that surpasses nnUNet.</font>
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.07423">
                            <b> Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations </b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Shuang Yu*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.07423" target="_blank">Paper</a>, <a href="https://github.com/funnyzhou/C2L_MICCAI2020" target="_blank">Code</a>]
                            <br><font color="red">The first contrastive pre-training method in medical image analysis.</font>
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div> 
          </div>
<!-- 	<h2>Publications</h2> -->

<!-- 	<table class="pub_table">
	<tbody>

       <tr>
           <td class="pub_td1"><img src="./zhouhy_files/c2l.png" class="papericon"></td>
           <td class="pub_td2"><u>Hong-Yu Zhou*</u>, Shuang Yu*, Cheng Bian, Yifan Hu, Kai Ma and Yefeng Zheng<br><b>Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By Comparing Image Representations</b><br>MICCAI 2020<br>[<a href="https://arxiv.org/abs/2007.07423">PDF</a>][<a href="https://github.com/funnyzhou/C2L_MICCAI2020">code</a>]
         </td></tr>  

       <tr>
           <td class="pub_td1"><img src="./zhouhy_files/miccai1015.png" class="papericon"></td>
           <td class="pub_td2">Shuang Yu*, <u>Hong-Yu Zhou*</u>, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu and Yefeng Zheng<br><b>Difficulty-aware Glaucoma Classification with Multi-Rater Consensus Modeling</b><br>MICCAI 2020<br>[<a href="https://arxiv.org/abs/2007.14848">PDF</a>]
         </td></tr>  
       <tr>
           <td class="pub_td1"><img src="./zhouhy_files/miccai1195.png" class="papericon"></td>
           <td class="pub_td2">Munan Ning, Cheng Bian, Donghuan Lu, <u>Hong-Yu Zhou</u>, Shuang Yu, Chenglang Yuan, Yang Guo, Yaohua Wang, Kai Ma and Yefeng Zheng<br><b>A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation</b><br>MICCAI 2020<br>[<a href="https://arxiv.org/abs/2007.10007">PDF</a>]
         </td></tr>  
       <tr>
           <td class="pub_td1"><img src="./zhouhy_files/ssl-vs-pretrained.png" class="papericon"></td>
           <td class="pub_td2"><u>Hong-Yu Zhou</u>, Avital Oliver, Jianxin Wu, and Yefeng Zheng<br><b>When Semi-Supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets</b><br>arXiv, 2018<br>[<a href="https://arxiv.org/pdf/1812.05313.pdf">PDF</a>][<a href="https://github.com/funnyzhou/ssl-vs-pretrained-models">code</a>]
         </td></tr>  

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/thinet.png" class="papericon"></td>
           <td class="pub_td2">Jian-Hao Luo, Hao Zhang, <u>Hong-Yu Zhou</u>, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin<br><b>ThiNet: Pruning CNN Filters for a Thinner Net</b><br> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>[<a href="https://ieeexplore.ieee.org/document/8416559/">PDF</a>] [<a href="http://lamda.nju.edu.cn/luojh/project/ThiNet_ICCV17/ThiNet_ICCV17.html">Project</a>]
         </td></tr>
       <tr>
           <td class="pub_td1"><img src="./zhouhy_files/vortex.png" class="papericon"></td>
           <td class="pub_td2">Chen-Wei Xie, <u>Hong-Yu Zhou</u>, and Jianxin Wu<br><b>Vortex Pooling: Improving Context Representation in Semantic Segmentation</b><br> arXiv:1804.06242, 2018<br>[<a href="https://arxiv.org/pdf/1804.06242.pdf">PDF</a>]
         </td></tr>

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/ijcai18.png" class="papericon"></td>
           <td class="pub_td2">Bin-Bin Gao, <u>Hong-Yu Zhou</u>, Jianxin Wu, and Xin Geng<br><b>Age Estimation Using Expectation of Label Distribution Learning</b><br>The 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI), 2018<br>[<a href="http://lamda.nju.edu.cn/gaobb/Projects/DLDL-v2.html">Project</a>]
         </td></tr>

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/code_attention.png" class="papericon"></td>
           <td class="pub_td2">Wen-Hao Zheng, <u>Hong-Yu Zhou</u>, Ming Li, and Jianxin Wu<br><b>Code Attention: Translating Code to Comments by Exploiting Domain Features</b><br>Frontiers of Computer Science<br>[<a href="https://arxiv.org/pdf/1709.07642.pdf">PDF</a>]
         </td></tr>

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/af.png" class="papericon"></td>
           <td class="pub_td2"><u>Hong-Yu Zhou</u>, Bin-Bin Gao, and Jianxin Wu<br><b>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</b><br>The IEEE International Conference on Computer Vision (ICCV), 2017<br>[<a href="https://arxiv.org/pdf/1707.06399.pdf">PDF</a>][<a href="./Projects/AF.html">Project</a>][<a href="./videos/skyfall_piece.mp4">Demo</a>]
         </td></tr>          

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/ss.png" class="papericon"></td>
           <td class="pub_td2"><u>Hong-Yu Zhou</u>, Bin-Bin Gao, and Jianxin Wu<br><b>Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition</b><br>The 28th British Machine Vision Conference (BMVC), 2017<br>[<a href="https://arxiv.org/pdf/1707.06335.pdf">PDF</a>][<a href="https://mega.nz/#!s8IhgJCC!nSDluCHXNQddrb7Sh_JG3AvIeP2XwhAU66_im_7FByY">Dataset (2.12G)</a>][<a href="./Projects/sos.html">Project</a>]
         </td></tr> 

        <tr>
           <td class="pub_td1"><img src="./zhouhy_files/image_recover.png" class="papericon"></td>
           <td class="pub_td2"><u>Hong-Yu Zhou</u> and Jianxin Wu<br><b>Content-Based Image Recovery</b><br>The 2017 Pacific-Rim Conference on Multimedia (PCM), 2017<br>[<a href="./pdf_files/image_recover.pdf">PDF</a>]
         </td></tr>

	</tbody></table> -->
    <div class="text_container visible">
        <div class="award-anchor" id="award-anchor"></div>
        <h2 name='award' id='award'>Selected Awards</h2>
        <div>
            <ul>
                <li><b>First place</b> in AASCE 2019, held by MICCAI 2019. The proposed algorithm is able to estimate the spinal curvature in Adolescent Idiopathic Scoliosis (AIS) accurately. Press coverage: <a href="https://tech.china.com/article/20190929/kejiyuan0129377260.html">中华网</a> <a href="http://www.kejixun.com/article/190929/474694.shtml">科技讯</a> <a href=" http://yisheng.12120.net/news/jkpt/content_0910114529.html">中国好医生网</a> <a href="https://tech.ifeng.com/c/7qMGUlwISFH">凤凰网</a>.</li>
                <li><b>First place</b> in Segthor 2019 Challenge, held by ISBI 2019. The proposed approach aims at segmenting organs at risk segmentation in Computed Tomography (CT) images. We achieved the highest Dice scores on Esophagus, Heart and Trachea. Press coverage: <a href="https://tech.qq.com/a/20190619/003614.htm">腾讯科技</a> <a href="https://www.jiqizhixin.com/articles/2019-06-19-8">机器之心</a> <a href="https://tech.huanqiu.com/article/9CaKrnKl1aA">环球网</a> <a href="https://www.qbitai.com/2019/06/3231.html">量子位</a> <a href="https://www.sohu.com/a/322090752_610300">搜狐网</a>.</li>
                <li><b>First place</b> in AI4Health Challeng, held by Thales group which provides solutions for over 50% of worldwide radiology examinations. In this challenge, we proposed a novel methodology to detect key points in X-rays very accurately (the mean error is smaller than <b>0.3 pixel</b>). Press coverage: <a href="http://www.ce.cn/xwzx/gnsz/gdxw/201907/12/t20190712_32605039.shtml">中国经济网</a> <a href="https://tech.huanqiu.com/article/9CaKrnKluBC">环球网</a> <a href="https://cloud.tencent.com/developer/article/1600187">腾讯新闻</a> <a href=""> <a href="http://www.geekpark.net/news/244645">极客公园</a> <a href="http://news.eastday.com/eastday/13news/auto/news/china/20190712/u7ai8688252.html">东方网</a>. </li>
                <li><b>First places</b> in all three tracks in 2019 MICCAI AGE Challenge.</li>
                <li><b>Second place</b> in ODIR 2019, held by Peking University.</li>
            </ul>
        </div>
    </div>


    <h2>Academic Service</h2>
    I regularly serve as reviewers for following conferences and journals.
    <ul>
	    <li>Conference: for AAAI, CVPR, ICLR, ICCV, MICCAI, and NeurIPS.</li>
      <li>Journal: for TMI, TKDE, TIP, MedIA, and Scientific Reports.</li>
    </ul>
</div>
</div>

</body></html>
