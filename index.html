<!-- saved from url=(0031)https://funnyzhou.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hong-Yu Zhou</title>

<link rel="stylesheet" href="./zhouhy_files/bootstrap.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script src="./zhouhy_files/jquery-3.1.1.slim.min.js"></script>

<style type="text/css">
 @import url("https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");
 /* @font-face {
    font-family: custom-font;
    src: url(./CAVOLINI.TTF);
  } */
  body
  {
  	font-family: Sans-serif;
      background-color : #CDCDCD;
      font-size: 18px;
  }
      .content
  	{
      		width : 900px;
      		padding : 25px 30px;
      		margin : 25px auto;
      		background-color : #fff;
      		box-shadow: 0px 0px 10px #999;
      		border-radius: 15px; 
  	}	
  	table
  	{
  		padding: 5px;
  	}
  	
  	table.pub_table,td.pub_td1,td.pub_td2
  	{
  		padding: 8px;
  		width: 850px;
          border-collapse: separate;
          border-spacing: 15px;
          margin-top: -5px;
  	}

  	td.pub_td1
  	{
  		width:50px;
  	}
      td.pub_td1 img
      {
          height:120px;
          width: 160px;
      }
  	
  	div#container
  	{
  		margin-left: auto;
  		margin-right: auto;
  		width: 820px;
  		text-align: left;
  		position: relative;
  		background-color: #FFF;
  	}
  	div#DocInfo
  	{
  		color: #1367a7;
  		height: 158px;
  	}
  	h4,h3,h2,h1
  	{
  		color: #3B3B3B;
  	}
  	h2
  	{
  		font-size:130%;
  	}
  	p
  	{
  		color: #5B5B5B;
  		margin-bottom: 50px;
  	}
  	p.caption
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 600px;
  	}
  	p.caption2
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 800px;
  	}
  	#header_img
  	{
  		position: absolute;
  		top: 0px; right: 0px;
      }
  	a:link,a:visited
  	{
  		color: #1367a7;
  		text-decoration: none;
  	}

      #hku_logo {
          position: absolute;
          left: 646px;
          top: 14px;
          width: 200px;
          height: 20px;
      }
     
      table.pub_table tr {
          outline: thin dotted #666666;
      }
      .papericon {
          border-radius: 8px; 
          -moz-box-shadow: 3px 3px 6px #888;
          -webkit-box-shadow: 3px 3px 6px #888;
          box-shadow: 3px 3px 6px #888;
      }
      .one{
        margin-bottom: 1em;
      }
      .two{
        margin-top: 1em;
      }
      .media {
          margin-bottom: 15px;
          margin-left: 10px;
      }

      .media-body {
          margin-top: 5px;
          padding-left: 20px;
      }

      .publication {
          margin-bottom: 15px;
      }

      .papers-selected .publication {
          display: none;
      }

      .papers-selected .book-chapters {
          display: none;
      }

      .papers-selected #show-selected {
          color: black;
          text-decoration: underline;
      }

      .papers-selected .paperhi {
          display: flex;
      }

      .papers-selected .paper-year {
          display: none;
      }

      .papers-by-date #show-by-date {
          color: black;
          text-decoration: underline;
      }

      .papers-by-date .paper-selected {
          display: none;
      }

      .papers-by-date .book-chapters {
          display: none;
      }

      .book-chapters #book-chapters {
          color: black;
          text-decoration: underline;
      }

      .book-chapters .paper-selected,
      .book-chapters .paper-year,
      .book-chapters .publication {
          display: none;
      }

      .book-chapters .chapter {
          display: flex;
      }

      /* .papers-by-date .paperhi {
          display: none;
      } */

      .hidden>div {
          display: none;
      }

      .visible>div {
          display: block;
      }
</style>

<!-- <script type="text/javascript" async="" src="./zhouhy_files/ga.js"></script><script async="" src="./zhouhy_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23931362-2', 'nju.edu.cn');
  ga('send', 'pageview');

</script> -->
<!-- <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23931362-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

    var myPix = new Array("zhouhy_files/me_hk.png")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };

</script> -->
<script>
  $(document).ready(function () {
            $('#show-selected').click(function () {
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').addClass('papers-selected');
            });

            $('#show-by-date').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').addClass('papers-by-date');
            });

            $('#book-chapters').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('papers-by-date');
            });

            $('.papers-container').addClass('papers-selected');


            $('.text_container').addClass("hidden");
            $('.text_container').click(function () {
                var $this = $(this);

                if ($this.hasClass("hidden")) {
                    $(this).removeClass("hidden").addClass("visible");

                } else {
                    $(this).removeClass("visible").addClass("hidden");
                }
            });

  });
</script>
</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="./zhouhy_files/me_hk.jpg" style="float:left; margin-right: 20px" height="190px" class="papericon"></td>
	<td>
	<div id="DocInfo">
		<h1>Hong-Yu Zhou</h1>
        <font>PhD Candidate</font><br>
        <font>Department of Computer Science</font><br>
        <font>Advisor: Yizhou Yu</font><br>
        <font>Office: Rm 416 (14), Chow Yei Ching Building</font><br> 
        <font>Email Address:</font><a href="mailto:whuzhouhongyu@gmail.com"> <font>whuzhouhongyu at gmail </font></a><br>
        <a href="https://twitter.com/HongYuZhou14" alt="twitter"><img width="50" src="twitter.gif"></img></a><br> 
	</div><br>
        <div id="hku_logo">
        <a href="https://www.hku.hk/"><img src="./zhouhy_files/hku_icon.jpg" height="170px" class="papericon"></a>
    </div>
	</td>
	</tr>
	</tbody></table>

	<h2>Bio</h2>
    <ul>
        <li>Now, I am a third-year PhD candidate at <a href="https://www.cs.hku.hk/">Department of Computer Science</a>, <a href="">The University of Hong Kong</a>. My advisor is Prof. <a href="https://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>.</li>
		<li> My research interests lie primarily in the intersection of artificial intelligence, medicine, and healthcare. The long-term goal is to improve individual healthcare with machine intelligence and medical professionalism. 
        <li> At present, I am especially interested in <b>Artificial Intelligence (AI) for Biomedical Discovery</b>, where topics that interest me the most involve:
        <ul>
            <li> <i>F</i>oundation Models that can perform a wide range of biomedical tasks,</li>
            <li> <i>D</i>iscovering relationships between early observations (e.g., symptoms) and late-stage outcomes (e.g., cancer) with machine learning, </li>
            <li> <i>A</i>I-based personalized health monitoring and management.</li>
        </ul>
        <li>I got my master degree from <a href="http://www.lamda.nju.edu.cn/MainPage.ashx">LAMDA Group</a> at <a href="https://www.nju.edu.cn/en/main.psp">Nanjing University</a>, advised by Prof. <a href="https://cs.nju.edu.cn/wujx/">Jianxin Wu</a>. Before that, I received my bachelor degree from <a href="https://en.whu.edu.cn/">Wuhan University</a>. I was a senior researcher at Tencent focusing on Medical x AI applications, where my group leader is Dr. <a href="https://sites.google.com/site/yefengzheng/">Yefeng Zheng</a>.</li>
    </ul>
	
    <h2>Recent News</h2>
    <ul>
      <li>[2023/07] Received Foundation Publication Award (HKU, 2023).</li>
      <li>[2023/06] nnFormer was accepted by TIP.</li>
      <li>[2023/05] Received ICMA PhD Fellowship (5 Recipients worldwide).</li>
      <li>[2023/04] <img alt="New!" width="50" src="R.gif">One paper was accepted by Nature Biomedical Engineering.</li>
      <li>[2023/01] <img alt="New!" width="50" src="R.gif">Two papers were accepted by ICLR. </li>
    </ul>
      <div class="papers-container">
          <div class="publication-anchor" id="publication-anchor"></div>
          <h2 class="paperlo">Paper <span id="paper-show">(<span id="show-selected">Selected (Highlight)</span> /
                  <span id="show-by-date">Full</span>)</span>
          </h2>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2023</p></h3>
          <div class="paper-2023">  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="">
                            <b> A Transformer-based Representation-learning Model with Unified Processing of Multimodal Input for Clinical Diagnostics.</b></a>
                            <br> Hong-Yu Zhou*, Yizhou Yu*, Chengdi Wang*, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao, Guangming Lu, Kang Zhang, Weimin Li.
                            <br>[<a href="https://arxiv.org/abs/2306.00864">Paper</a>, <a href="https://github.com/RL4M/IRENE" target="_blank">Code</a>]
                            <!-- <br><i class="fa fa-lightbulb-o" style="font-size:24px"></i> <font color="red"> A multimodal transformer for clinical diagnostics.</font> -->
                            <br><i>Nature Biomedical Engineering, 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2301.13155">
                            <b> Advancing Radiograph Representation Learning with Masked Record Modeling</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chenyu Lian*, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2301.13155">Paper</a>, <a href="https://github.com/RL4M/MRM-pytorch" target="_blank">Code</a>]
                            <br><i>International Conference on Learning Representations (ICLR), 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2301.13154">
                            <b> Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Yunxiang Fu*, Zhicheng Zhang, Cheng Bian, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2301.13154">Paper</a>, <a href="https://github.com/RL4M/KeAP" target="_blank">Code</a>]
                            <br><i>International Conference on Learning Representations (ICLR), 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                      <div class="media-body ">
                          <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                          <b> A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis</b></a>
                          <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Chaoqi Chen, Sibei Yang, Yizhou Yu.
                          <br>[<a href="https://arxiv.org/abs/2301.00772">Paper</a>, <a href="https://github.com/RL4M/PCRLv2" target="_blank">Code</a>]
                          <br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.</i>
                          <br>
                      </div>
                    </div>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2022</p></h3>
          <div class="paper-2022">  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> Development and validation of an abnormality-derived deep-learning diagnostic system for major respiratory diseases</b></a>
                            <br> Chengdi Wang, Jiechao Ma, Shu Zhang, Jun Shao, Yanyan Wang, <b>Hong-Yu Zhou</b>, Lujia Song, Jie Zheng, Yizhou Yu, Weimin Li
                            <br>[<a href="https://www.nature.com/articles/s41746-022-00648-z">Paper</a>, <a href="https://github.com/chiehchiu/DeepMRD" target="_blank">Code</a>]
                            <br><i>npj digital medicine, 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification</b></a>
                            <br> Zhixiong Yang*, Junwen Pan*, Yanzhan Yang, Xiaozhou Shi, <b>Hong-Yu Zhou</b>, Zhicheng Zhang, Cheng Bian
                            <br>[<a href="https://arxiv.org/abs/2206.02355">Paper</a>, <a href="https://github.com/skyz215/ProCo" target="_blank">Code</a>]
                            <br><i>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> Relation Matters: Foreground-aware Graph-based Relational Reasoning for Domain Adaptive Object Detection</b></a>
                            <br> Chaoqi Chen, Jiongcheng Li, <b>Hong-Yu Zhou</b>, Xiaoguang Han, Yue Huang, Xinghao Ding, Yizhou Yu
                            <br>[<a href="https://arxiv.org/abs/2206.02355">Paper</a>, <a href="" target="_blank">Code</a>]
                            <br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="">
                            <b> GraVIS: Grouping Augmented Views from Independent Sources for Dermatology Analysis</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Liansheng Wang, Yizhou Yu
                            <br>[<a href="">Paper</a>, <a href="" target="_blank">Code</a>]
                            <br><i>IEEE Transactions on Medical Imaging (TMI), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522001530">
                            <b> MASS: Modality-collaborative semi-supervised segmentation by exploiting cross-modal consistency from unpaired CT and MRI images</b></a>
                            <br> Xiaoyu Chen*, <b>Hong-Yu Zhou</b>*, FengLiu, Jiansen Guo, Liansheng Wang, Yizhou Yu
                            <br>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522001530">Paper</a>, <a href="https://github.com/xy123chen/MASS" target="_blank">Code</a>]
                            <br><i>Medical Image Analysis, 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2203.09064">
                            <b> Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning</b></a>
                            <br> Yangji He, Weihan Liang, Dongyang Zhao, <b>Hong-Yu Zhou</b>, Weifeng Ge, Yizhou Yu, Wenqiang Zhang
                            <br>[<a href="https://arxiv.org/pdf/2203.09064.pdf">Paper</a>, <a href="https://github.com/StomachCold/HCTransformers" target="_blank">Code</a>]
                            <br><i>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2022.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2201.01426">
                            <b> Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training</b></a>
                            <br> Shu Zhang, Zihao Li, <b>Hong-Yu Zhou</b>, Jiechao Ma, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2201.01426" target="_blank">Paper</a>, <a href="https://github.com/urmagicsmine/CSPR" target="_blank">Code</a>]
                            <br><i>Technical Report.</i>
                            <br>
                        </div>
                    </div>
          	        <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2111.03452">
                            <b> Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Xiaoyu Chen*, Yinghao Zhang*, Ruibang Luo, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://www.nature.com/articles/s42256-021-00425-9">Nature MI</a>, <a href="https://arxiv.org/abs/2111.03452" target="_blank">arXiv</a>, <a href="https://github.com/funnyzhou/REFERS" target="_blank">Code</a>]
                            <br><i>Nature Machine Intelligence, 2022.</i>
                            <br>
                        </div>
                    </div>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2021</p></h3>
          <div class="paper-2021">    
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chixiang Lu*, Sibei Yang, Xiaoguang Han, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.04379" target="_blank">Paper</a>, <a href="https://github.com/Luchixiang/PCRL" target="_blank">Code</a>]
                            <br><i> International Conference on Computer Vision (ICCV), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> nnFormer: Interleaved Transformer for Volumetric Segmentation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Jiansen Guo*, Yinghao Zhang*, Lequan Yu, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.03201" target="_blank">Paper</a>, <a href="https://github.com/282857341/nnFormer" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2108.05305">
                            <b> ConvNets vs. Transformers: Whose Visual Representations are More Transferable?</b></a>
                            <br> <b>Hong-Yu Zhou</b>, Chixiang Lu, Sibei Yang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2108.05305" target="_blank">Paper</a>]
                            <br><i> ICCV workshop on Multi-Task Learning in Computer Vision (DeepMTL). </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.01755">
                            <b> Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition</b></a>
                            <br> Bin-Bin Gao, <b>Hong-Yu Zhou</b>.
                            <br>[<a href="https://arxiv.org/abs/2007.01755" target="_blank">Paper</a>, <a href="https://github.com/gaobb/MCAR" target="_blank">Code</a>]
                            <br><i> IEEE Transactions on Image Processing (TIP), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2103.16344">
                            <b> Generalized Organ Segmentation by Imitating One-Shot Reasoning Using Anatomical Correlation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Hualuo Liu*, Shilei Cao*, Dong Wei, Chixiang Lu, Yizhou Yu, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2103.16344" target="_blank">Paper</a>]
                            <br><i> International Conference on Information Processing in Medical Imaging (IPMI), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.medrxiv.org/content/10.1101/2021.06.02.21258256v1">
                            <b> SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency and Heterogeneous Perturbation</b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Chengdi Wang*, Haofeng Li*, Gang Wang, Shu Zhang, Weimin Li, Yizhou Yu.
                            <br>[<a href="https://www.medrxiv.org/content/10.1101/2021.06.02.21258256v1" target="_blank">Paper</a>]
                            <br><i> Medical Image Analysis, 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://www.sciencedirect.com/science/article/pii/S1361841521001614">
                            <b> Evaluation and Comparison of Accurate Automated Spinal Curvature Estimation Algorithms with Spinal Anterior-posterior X-Ray Images: The AASCE2019 Challenge</b></a>
                            <br> Liansheng Wang, Cong Xie, Yi Lin, <b>Hong-Yu Zhou</b>, Kailin Chen, Dalong Cheng, Florian Dubost, Benjamin Collery, Bidur Khanal, Bishesh Khanal, Rong Tao, Shangliang Xu, Upasana Upadhyay Bharadwaj, Zhusi Zhong, Jie Li, Shuxin Wang, Shuo Li.
                            <br>[<a href="https://www.sciencedirect.com/science/article/pii/S1361841521001614" target="_blank">Paper</a>]
                            <br><i> Medical Image Analysis, 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2107.08715">
                            <b> Recist-Net: Lesion Detection via Grouping Keypoints on Recist-Based Annotation</b></a>
                            <br> Cong Xie, Shilei Cao, Dong Wei, <b>Hongyu Zhou</b>, Kai Ma, Xianli Zhang, Buyue Qian, Liansheng Wang, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/pdf/2107.08715.pdf" target="_blank">Paper</a>]
                            <br><i> IEEE 18th International Symposium on Biomedical Imaging (ISBI), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2102.13280">
                            <b> MixSearch: Searching for Domain Generalized Medical Image Segmentation Architectures</b></a>
                            <br> Luyan Liu*<sup>,&#8224;</sup>, Zhiwei Wen*, Songwei Liu*, <b>Hong-Yu Zhou</b>*<sup>,&#8224;</sup>, Hongwei Zhu, Weicheng Xie, Linlin Shen, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2102.13280" target="_blank">Paper</a>, <a href="https://github.com/lswzjuer/NAS-WDAN/" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf">
                            <b> Bottom-Up Shift and Reasoning for Referring Image Segmentation</b></a>
                            <br> Sibei Yang, Meng Xia, Guanbin Li, <b>Hong-Yu Zhou</b>, Yizhou Yu.
                            <br>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf" target="_blank">Paper</a>, <a href="https://github.com/incredibleXM/BUSNet" target="_blank">Code</a>]
                            <br><i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. </i>
                            <br>
                        </div>
                    </div>       
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2020</p></h3>
          <div class="paper-2020">
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.10007">
                            <b> A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation</b></a>
                            <br> Munan Ning, Cheng Bian, Donghuan Lu, <b>Hong-Yu Zhou</b>, Shuang Yu, Chenglang Yuan, Yang Guo, Yaohua Wang, Kai Ma, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.10007" target="_blank">Paper</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>   
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.14848">
                            <b> Difficulty-aware Glaucoma Classification with Multi-rater Consensus Modeling </b></a>
                            <br> Shuang Yu*, <b>Hong-Yu Zhou</b>*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.14848" target="_blank">Paper</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.07423">
                            <b> Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations </b></a>
                            <br> <b>Hong-Yu Zhou</b>*, Shuang Yu*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.07423" target="_blank">Paper</a>, <a href="https://github.com/funnyzhou/C2L_MICCAI2020" target="_blank">Code</a>]
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div>          
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://web.archive.org/web/20210429012105id_/https://ieeexplore.ieee.org/ielx7/6221020/9216186/09172136.pdf">
                            <b> Efficient and Effective Training of Covid-19 Classification Networks with Self-supervised Dual-track Learning to Rank </b></a>
                            <br> Yuexiang Li, Dong Wei, Jiawei Chen, Shilei Cao, <b>Hongyu Zhou</b>, Yanchun Zhu, Jianrong Wu, Lan Lan, Wenbo Sun, Tianyi Qian, Kai Ma, Haibo Xu, Yefeng Zheng.
                            <br>[<a href="https://web.archive.org/web/20210429012105id_/https://ieeexplore.ieee.org/ielx7/6221020/9216186/09172136.pdf" target="_blank">Paper</a>]
                            <br><i> IEEE Journal of Biomedical and Health Informatics (JBHI), 2020. </i>
                            <br>
                        </div>
                    </div>         
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.01771">
                            <b> Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation </b></a>
                            <br> Bin-Bin Gao, Xin-Xin Liu, <b>Hong-Yu Zhou</b>, Jianxin Wu, Xin Geng.
                            <br>[<a href="https://arxiv.org/abs/2007.01771" target="_blank">Paper</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>
          <h3 class="pt-2 pb-1 paper-year"><p style="color:rgb(20, 68, 106); display:inline">2019 and Before</p></h3>
          <div class="paper-2019">
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://link.springer.com/chapter/10.1007/978-3-030-39752-4_7">
                            <b> Seg4Reg Networks for Automated Spinal Curvature Estimation </b></a>
                            <br> Yi Lin, <b>Hong-Yu Zhou</b><sup>&#8224;</sup>, Kai Ma, Xin Yang, Yefeng Zheng.
                            <br>[<a href="https://link.springer.com/chapter/10.1007/978-3-030-39752-4_7" target="_blank">Paper</a>]
                            <br><font color="red">Ranked 1st place in 2019 MICCAI AASCE Challenge.</font>
                            <br><i> Computational Methods and Clinical Applications for Spine Imaging, 2019. </i>
                            <br>
                        </div>
                    </div>    
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://cs.nju.edu.cn/wujx/paper/FCS2018_CodeAttention.pdf">
                            <b> CodeAttention: Translating Source Code to Comments by Exploiting the Code Constructs </b></a>
                            <br> Wenhao Zheng, <b>Hongyu Zhou</b>, Ming Li, Jianxin Wu.
                            <br>[<a href="https://cs.nju.edu.cn/wujx/paper/FCS2018_CodeAttention.pdf" target="_blank">Paper</a>]
                            <br><i> Frontiers of Computer Science, 2019. </i>
                            <br>
                        </div>
                    </div>         
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1812.05313">
                            <b> When Semi-supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Avital Oliver, Jianxin Wu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/1812.05313" target="_blank">Paper</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>       
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://ieeexplore.ieee.org/abstract/document/8416559">
                            <b> ThiNet: Pruning CNN Filters for a Thinner Net </b></a>
                            <br> Jian-Hao Luo, Hao Zhang, <b>Hong-Yu Zhou</b>, Chen-Wei Xie, Jianxin Wu, Weiyao Lin.
                            <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8416559" target="_blank">Paper</a>, <a href="https://github.com/Roll920/ThiNet_Code" target="_blank">Code</a>]
                            <br><i> IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI), 2018. </i>
                            <br>
                        </div>
                    </div>   
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="http://palm.seu.edu.cn/xgeng/files/ijcai18d.pdf">
                            <b> Age Estimation Using Expectation of Label Distribution Learning </b></a>
                            <br> Bin-Bin Gao, <b>Hong-Yu Zhou</b>, Jianxin Wu, Xin Geng.
                            <br>[<a href="http://palm.seu.edu.cn/xgeng/files/ijcai18d.pdf" target="_blank">Paper</a>, <a href="https://github.com/gaobb/DLDL-v2" target="_blank">Code</a>]
                            <br><i> International Joint Conference on Artificial Intelligence (IJCAI), 2018. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1804.06242">
                            <b> Vortex Pooling: Improving Context Representation in Semantic Segmentation </b></a>
                            <br> Chen-Wei Xie, <b>Hong-Yu Zhou</b>, Jianxin Wu.
                            <br>[<a href="https://arxiv.org/abs/1804.06242" target="_blank">Paper</a>,  <a href="https://github.com/MTCloudVision/deeplabv3-mxnet_gluon" target="_blank">Code</a>]
                            <br><i> Technical Report. </i>
                            <br>
                        </div>
                    </div>  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/1707.06335">
                            <b> Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Bin-Bin Gao, Jianxin Wu.
                            <br>[<a href="https://arxiv.org/abs/1707.06335" target="_blank">Paper</a>]
                            <br><i> The British Machine Vision Association (BMVC), 2017. </i>
                            <br>
                        </div>
                    </div>  
                    <div class="publication media">
                        <div class="media-body ">
                            <a class="paper-title" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper.pdf">
                            <b> Adaptive feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors </b></a>
                            <br> <b>Hong-Yu Zhou</b>, Bin-Bin Gao, Jianxin Wu.
                            <br>[<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Adaptive_Feeding_Achieving_ICCV_2017_paper.pdf" target="_blank">Paper</a>,  <a href="https://github.com/funnyzhou/Adaptive_Feeding" target="_blank">Code</a>]
                            <br><i> International Conference on Computer Vision (ICCV), 2017. </i>
                            <br>
                        </div>
                    </div>  
          <div class="paper-selected">
          <br>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="">
                            <b> A Transformer-based Representation-learning Model with Unified Processing of Multimodal Input for Clinical Diagnostics.</b></a>
                            <br> Hong-Yu Zhou*, Yizhou Yu*, Chengdi Wang*, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao, Guangming Lu, Kang Zhang, Weimin Li.
                            <br>[<a href="https://t.co/096K7GQf9B">Nature BME</a>, <a href="https://arxiv.org/abs/2306.00864">arXiv</a>, <a href="https://github.com/RL4M/IRENE" target="_blank">Code</a>]
                            <br><i class="fa fa-lightbulb-o" style="font-size:24px"></i> <font color="red"> A multimodal transformer for clinical diagnostics.</font>
                            <br><i>Nature Biomedical Engineering, 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2301.13155">
                            <b> Advancing Radiograph Representation Learning with Masked Record Modeling.</b></a>
                            <br> Hong-Yu Zhou*, Chenyu Lian*, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2301.13155">Paper</a>, <a href="https://github.com/RL4M/MRM-pytorch" target="_blank">Code</a>]
                            <br><i class="fa fa-lightbulb-o" style="font-size:24px"></i> <font color="red"> Masked + language modeling for visual representation learning.</font>
                            <br><i>International Conference on Learning Representations (ICLR), 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2301.13154">
                            <b> Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling.</b></a>
                            <br> Hong-Yu Zhou*, Yunxiang Fu*, Zhicheng Zhang, Cheng Bian, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2301.13154">Paper</a>, <a href="https://github.com/RL4M/KeAP" target="_blank">Code</a>]
                            <br><i class="fa fa-lightbulb-o" style="font-size:24px"></i><font color="red"> Masked language modeling on knowledge graphs.</font>
                            <br><i>International Conference on Learning Representations (ICLR), 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis.</b></a>
                            <br> Hong-Yu Zhou*, Chixiang Lu*, Chaoqi Chen, Sibei Yang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2301.00772">Paper</a>, <a href="https://github.com/RL4M/PCRLv2" target="_blank">Code</a>]
                            <br><i class="fa fa-lightbulb-o" style="font-size:24px"></i><font color="red"> New state-of-the-arts of self-supervised learning on 2D and 3D medical data.</font>
                            <br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer.</b></a>
                            <br> Hong-Yu Zhou*, Jiansen Guo*, Yinghao Zhang*, Xiaoguang Han, Lequan Yu, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.03201" target="_blank">Paper</a>, <a href="https://github.com/282857341/nnFormer" target="_blank">Code</a>]
                            <!-- <br><font color="red">A transformer-based segmentation network that surpasses nnUNet.</font> -->
                            <br><i> IEEE Transactions on Image Processing, 2023. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2111.03452">
                            <b> Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports.</b></a>
                            <br> Hong-Yu Zhou*, Xiaoyu Chen*, Yinghao Zhang*, Ruibang Luo, Liansheng Wang, Yizhou Yu.
                            <br>[<a href="https://www.nature.com/articles/s42256-021-00425-9">Nature MI</a>, <a href="https://arxiv.org/abs/2111.03452" target="_blank">arXiv</a>, <a href="https://github.com/funnyzhou/REFERS" target="_blank">Code</a>, <a href="https://www.asianscientist.com/2022/06/in-the-lab/revolutionizing-ai-enabled-medical-imaging" target="_blank">AsianScientist</a>, <a href="https://thepaper.cn/newsDetail_forward_17590325" target="_blank">ThePaper(澎湃新闻)</a>]
                            <!-- <br><font color="red">We propose cross-supervised learning to learn X-ray representations from associated radiology reports, which achieves great improvements over previous self-supervised methodologies.</font> -->
                            <br><i> Nature Machine Intelligence, 2022</i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts.</b></a>
                            <br> Hong-Yu Zhou*, Chixiang Lu*, Sibei Yang, Xiaoguang Han, Yizhou Yu.
                            <br>[<a href="https://arxiv.org/abs/2109.04379" target="_blank">Paper</a>, <a href="https://github.com/Luchixiang/PCRL" target="_blank">Code</a>]
                            <!-- <br><font color="red">New state-of-the-arts of self-supervised medical image pre-training on both 2D and 3D data.</font> -->
                            <br><i> International Conference on Computer Vision (ICCV), 2021. </i>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.07423">
                            <b> Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations.</b></a>
                            <br> Hong-Yu Zhou*, Shuang Yu*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.
                            <br>[<a href="https://arxiv.org/abs/2007.07423" target="_blank">Paper</a>, <a href="https://github.com/funnyzhou/C2L_MICCAI2020" target="_blank">Code</a>]
                            <!-- <br><font color="red">The first contrastive pre-training method in medical image analysis.</font> -->
                            <br><i> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2020. </i>
                            <br>
                        </div>
                    </div> 
          </div>

    <div class="text_container visible">
        <div class="award-anchor" id="award-anchor"></div>
        <h2 name='award' id='award'>Selected Awards</h2>
        <div>
            <ul>
                <li><b>First place</b> in AASCE 2019, held in conjunction with MICCAI 2019. We proposed an approach to accurately estimate the spinal curvature in Adolescent Idiopathic Scoliosis (AIS). Press coverage: <a href="https://tech.china.com/article/20190929/kejiyuan0129377260.html">中华网</a> <a href="http://www.kejixun.com/article/190929/474694.shtml">科技讯</a> <a href=" http://yisheng.12120.net/news/jkpt/content_0910114529.html">中国好医生网</a> <a href="https://tech.ifeng.com/c/7qMGUlwISFH">凤凰网</a>.</li>
                <li><b>First place</b> in Segthor 2019 Challenge, held in conjunction with ISBI 2019. We proposed a method to segment organs at risk segmentation in Computed Tomography (CT) images. Press coverage: <a href="https://tech.qq.com/a/20190619/003614.htm">腾讯科技</a> <a href="https://www.jiqizhixin.com/articles/2019-06-19-8">机器之心</a> <a href="https://tech.huanqiu.com/article/9CaKrnKl1aA">环球网</a> <a href="https://www.qbitai.com/2019/06/3231.html">量子位</a> <a href="https://www.sohu.com/a/322090752_610300">搜狐网</a>.</li>
                <li><b>First place</b> in AI4Health Challenge, held by Thales group which provides solutions for over 50% of worldwide radiology examinations. In this challenge, we proposed a novel methodology to accurately detect key points in X-rays with a mean error smaller than <b>0.3 pixels</b>). Press coverage: <a href="http://www.ce.cn/xwzx/gnsz/gdxw/201907/12/t20190712_32605039.shtml">中国经济网</a> <a href="https://tech.huanqiu.com/article/9CaKrnKluBC">环球网</a> <a href="https://cloud.tencent.com/developer/article/1600187">腾讯新闻</a> <a href=""> <a href="http://www.geekpark.net/news/244645">极客公园</a> <a href="http://news.eastday.com/eastday/13news/auto/news/china/20190712/u7ai8688252.html">东方网</a>. </li>
                <li><b>Second place</b> in ODIR 2019, held by Peking University.</li>
            </ul>
        </div>
    </div>


    <h2>Academic Service</h2>
    I regularly serve as reviewers for following conferences and journals.
    <ul>
	    <li>Conference: for ICLR, ICML, NeurIPS, CVPR, ICCV, and MICCAI.</li>
      <li>Journal: for Nature Machine Intelligence, TPAMI, TMI, MedIA, TKDE, TMM, TIP, iScience (Cell family), and Scientific Reports.</li>
    </ul>
</div>
</div>

</body></html>
