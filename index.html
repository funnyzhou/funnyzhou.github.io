<!-- saved from url=(0031)https://funnyzhou.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hong-Yu Zhou</title>

<link rel="stylesheet" href="./zhouhy_files/bootstrap.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script src="./zhouhy_files/jquery-3.1.1.slim.min.js"></script>

<style>
    @keyframes blink {
      from, to { border-color: transparent }
      50% { border-color: black; }
    }
    #autoTyping {
      font-family: Avenir;
      border-right: 2px solid black;
      white-space: nowrap;
      overflow: hidden;
      animation: blink 0.75s step-end infinite;
    }
</style>

<style type="text/css">
 @import url("https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");
 /* @font-face {
    font-family: custom-font;
    src: url(./CAVOLINI.TTF);
  } */
  body
  {
  	font-family: Avenir;
      background-color : #CDCDCD;
      font-size: 16px;
  }
      .content
  	{
      		/* width : device-width; */
      		padding : 25px 30px;
      		margin : 0px auto;
      		background-color : #fff;
      		box-shadow: 0px 0px 10px #999;
      		border-radius: 15px; 
  	}	
  	table
  	{
  		padding: 5px;
  	}
  	
  	table.pub_table,td.pub_td1,td.pub_td2
  	{
  		padding: 8px;
  		width: 850px;
          border-collapse: separate;
          border-spacing: 15px;
          margin-top: -5px;
  	}

  	td.pub_td1
  	{
  		width:50px;
  	}
      td.pub_td1 img
      {
          height:120px;
          width: 160px;
      }
  	
  	div#container
  	{
  		margin-left: auto;
  		margin-right: auto;
  		width: 820px;
  		text-align: left;
  		position: relative;
  		background-color: #FFF;
  	}
  	div#DocInfo
  	{
  		color: #1367a7;
  		height: 158px;
  	}
  	h4,h3,h2,h1
  	{
  		color: #3B3B3B;
  	}
  	h2
  	{
  		font-size:130%;
  	}
  	p
  	{
  		color: #5B5B5B;
  		margin-bottom: 50px;
  	}
  	p.caption
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 600px;
  	}
  	p.caption2
  	{
  		color: #9B9B9B;
  		text-align: left;
  		width: 800px;
  	}
  	#header_img
  	{
  		position: absolute;
  		top: 0px; right: 0px;
      }
  	a:link,a:visited
  	{
  		/* color: #1367a7;  */
        color: black;
  		text-decoration: none;
  	}

      #hku_logo {
          position: absolute;
          left: 646px;
          top: 14px;
          width: 200px;
          height: 20px;
      }
     
      table.pub_table tr {
          outline: thin dotted #666666;
      }
      .papericon {
          border-radius: 8px; 
          -moz-box-shadow: 3px 3px 6px #888;
          -webkit-box-shadow: 3px 3px 6px #888;
          box-shadow: 3px 3px 6px #888;
      }
      .one{
        margin-bottom: 1em;
      }
      .two{
        margin-top: 1em;
      }
      .media {
          margin-bottom: 15px;
          margin-left: 10px;
      }

      .media-body {
          margin-top: 5px;
          padding-left: 20px;
      }

      .publication {
          margin-bottom: 15px;
      }

      .papers-selected .publication {
          display: none;
      }

      .papers-selected .book-chapters {
          display: none;
      }

      .papers-selected #show-selected {
          color: black;
          text-decoration: underline;
      }

      .papers-selected .paperhi {
          display: flex;
      }

      .papers-selected .paper-year {
          display: none;
      }

      .papers-by-date #show-by-date {
          color: black;
          text-decoration: underline;
      }

      .papers-by-date .paper-selected {
          display: none;
      }

      .papers-by-date .book-chapters {
          display: none;
      }

      .book-chapters #book-chapters {
          color: black;
          text-decoration: underline;
      }

      .book-chapters .paper-selected,
      .book-chapters .paper-year,
      .book-chapters .publication {
          display: none;
      }

      .book-chapters .chapter {
          display: flex;
      }

      /* .papers-by-date .paperhi {
          display: none;
      } */

      .hidden>div {
          display: none;
      }

      .visible>div {
          display: block;
      }
</style>

<script>
  $(document).ready(function () {
        $('#show-selected').click(function () {
            $('.papers-container').removeClass('papers-by-date');
            $('.papers-container').addClass('papers-selected');
        });

        $('.papers-container').addClass('papers-selected');

        $('.text_container').addClass("hidden");
        $('.text_container').click(function () {
            var $this = $(this);

            if ($this.hasClass("hidden")) {
                $(this).removeClass("hidden").addClass("visible");
            } else {
                $(this).removeClass("visible").addClass("hidden");
            }
        });

  });
</script>
</head>

<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="./zhouhy_files/me_hk.jpg" style="float:left; margin-right: 20px; margin-bottom: 20px;" height="170px" class="papericon"></td>
	<td>
	<div id="DocInfo">
		<font size="6" color="black">Hong-Yu Zhou</font><br>
        <font color="black">Postdoctoral fellow</font><br>
        <font color="black">Department of Biomedical Informatics</font><br>
        <font color="black">Harvard Medical School</font><br> 
        <font color="black"> Harvard University<font><br>
        <font color="black">Email:</font><a href="mailto:whuzhouhongyu@gmail.com"> <font>whuzhouhongyu at gmail.com </font></a> <br>
        <a href="https://twitter.com/HongYuZhou14" alt="twitter"><img width="42" src="twitter.png"></img></a><a href="https://scholar.google.com/citations?hl=zh-CN&user=aJnvh8gAAAAJ&view_op=list_works&sortby=pubdate" alt="gscholar"><img width="27" src="gscholar.png"></img></a><br> 
	</div><br>
        <!-- <div id="hku_logo"> -->
        <!-- <a href="https://www.hku.hk/"><img src="./zhouhy_files/hku_icon.jpg" height="170px" class="papericon"></a> -->
    </div>
	</td>
	</tr>
	</tbody></table>

	<!-- <h2>Bio</h2> -->
    <ul>
        <li> On the <strong style="color: #8B0000;">Job Market</strong> this year! Looking for positions in computational health, computer science, and related fields! Any contacts or opportunities are welcome!</li>
        <li> I am a postdoctoral fellow at the <a href="https://dbmi.hms.harvard.edu/">Harvard DBMI</a>, working with <a href="https://pranavrajpurkar.com/">Pranav Rajpurkar</a>.</li>
		<li> My research lies in the intersection of artificial intelligence, medicine, and data science.
        <li> I develop <span id="autoTyping"></span></li>
        <li> I am currently researching pathways to improve the scalability of medical AI systems:</li>
        <ul>
            <li> Health insights at scale: <font color="grey">How can one access a multitude of high-quality health insights (diagnoses, etc) at a notably reduced expense?</font></li>
            <li> Holistic multimodal analytics: <font color="grey">How can we leverage multimodal biomedical data (medical images, lab results, etc) to improve the modeling accuracy and to what extent can it do so?</font> </li>
            <li> Scaling system design: <font color="grey">Can we improve the modeling accuracy by increasing the data or model scale, for example, integrating large language models? How far can we go?</font> </li>
        </ul>
        <li>I finished my PhD at <a href="https://www.hku.hk/">The University of Hong Kong</a>, advised by <a href="https://i.cs.hku.hk/~yzyu/">Yizhou Yu</a> and my master degree at <a href="https://www.nju.edu.cn/en/">Nanjing University</a>, advised by <a href="https://cs.nju.edu.cn/wujx/">Jianxin Wu</a>. Before that, I received my undergraduate degree from <a href="https://en.whu.edu.cn/">Wuhan University</a>.</li>
    </ul>
	
    <h2>Recent Updates</h2>
    <ul>
        <!-- <img alt="New!" width="50" src="R.gif"> -->
      <li>[2024/09] Two papers &rarr; Nature Communications, One paper &rarr; Nature Medicine </li>
      <li>[2024/08] Three papers &rarr; MICCAI 2024 </li>  
      <li>[2024/07] One paper &rarr; MLHC 2024 </li>
      <li>[2024/06] Three papers (including Swin-UMamba) &rarr; MICCAI 2024 </li> 
      <li>[2024/05] One paper &rarr; MIDL 2024</a></li>
      <li>[2024/02] We released <a href="https://arxiv.org/abs/2402.03302">Swin-UMamba</a>!</li>
      <li>[2024/02] Three papers &rarr; ISBI 2024</li>
      <!-- <li>[2023/12] <a href="https://arxiv.org/abs/2310.16436">Multimodal Chain-of-Thought prompting</a> &rarr; NeurIPS 2023</li>
      <li>[2023/07] Foundation Publication Award (HKU, 2023)</li>
      <li>[2023/06] nnFormer &rarr; TIP</li>
      <li>[2023/05] ICMA PhD Fellowship (5 Recipients worldwide)</li>
      <li>[2023/04] One paper &rarr; Nature Biomedical Engineering</li>
      <li>[2023/01] Two papers &rarr; ICLR 2023 </li>  -->
    </ul>
      <div class="papers-container">
          <div class="publication-anchor" id="publication-anchor"></div>
          <h2 class="paperlo">Paper <span id="paper-show">(<span id="show-selected">Selected</span> /
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=aJnvh8gAAAAJ&view_op=list_works&sortby=pubdate" id="show-by-date">Full</a>)</span>
          </h2>
          <div class="paper-selected">
          <br>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="">
                            <b> A Transformer-based Representation-learning Model with Unified Processing of Multimodal Input for Clinical Diagnostics.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Yizhou Yu*, Chengdi Wang*, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao, Guangming Lu, Kang Zhang, Weimin Li.</font>
                            <br><i class="fa fa-lightbulb-o" style="font-size:20px"></i> <font color="red"> A multimodal transformer for clinical diagnostics.</font>
                            <br><a href="https://www.nature.com/natbiomedeng/?origen=app">Nature Biomedical Engineering 2023</a> | <a href="https://t.co/096K7GQf9B">Paper</a> | <a href="https://github.com/RL4M/IRENE" target="_blank">Code</a> | ESI - Top 1% highly cited papers
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2301.13155">
                            <b> Advancing Radiograph Representation Learning with Masked Record Modeling.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Chenyu Lian*, Liansheng Wang, Yizhou Yu.</font>
                            <br><i class="fa fa-lightbulb-o" style="font-size:20px"></i> <font color="red"> Multimodal modeling for visual representation learning.</font>
                            <br><a href="https://iclr.cc/Conferences/2023">ICLR 2023</a> | <a href="https://openreview.net/pdf?id=w-x7U26GM7j">Paper</a> | <a href="https://github.com/RL4M/MRM-pytorch?tab=readme-ov-file">Code</a>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Jiansen Guo*, Yinghao Zhang*, Xiaoguang Han, Lequan Yu, Liansheng Wang, Yizhou Yu.</font>
                            <br><i class="fa fa-lightbulb-o" style="font-size:20px"></i><font color="red"> A widely adopted baseline for 3D medical image segmentation.</font>
                            <br> <a href="https://ieeexplore.ieee.org/document/10183842?denied=">TIP 2023</a> | <a href="https://arxiv.org/abs/2109.03201">Paper</a> | <a href="https://github.com/282857341/nnFormer">Code</a> | ESI - Top 1% highly cited papers
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2206.02355">
                            <b> A Unified Visual Information Preservation Framework for Self-supervised Pre-training in Medical Image Analysis.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Chixiang Lu*, Chaoqi Chen, Sibei Yang, Yizhou Yu.</font>
                            <!-- <br><i class="fa fa-lightbulb-o" style="font-size:20px"></i><font color="red"> New state-of-the-arts of self-supervised learning on multimodal imaging data.</font> -->
                            <br> <a href="https://ieeexplore.ieee.org/document/10005161/">TPAMI 2023</a> | <a href="https://arxiv.org/pdf/2301.00772.pdf">Paper</a> | <a href="https://github.com/RL4M/PCRLv2">Code</a>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2111.03452">
                            <b> Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Xiaoyu Chen*, Yinghao Zhang*, Ruibang Luo, Liansheng Wang, Yizhou Yu.</font>
                            <!-- <br>[<a href="https://www.nature.com/articles/s42256-021-00425-9">Nature MI</a>, <a href="https://arxiv.org/abs/2111.03452" target="_blank">arXiv</a>, <a href="https://github.com/funnyzhou/REFERS" target="_blank">Code</a>, <a href="https://www.asianscientist.com/2022/06/in-the-lab/revolutionizing-ai-enabled-medical-imaging" target="_blank">AsianScientist</a>, <a href="https://thepaper.cn/newsDetail_forward_17590325" target="_blank">ThePaper(澎湃新闻)</a>] -->
                            <!-- <br><font color="red">We propose cross-supervised learning to learn X-ray representations from associated radiology reports, which achieves great improvements over previous self-supervised methodologies.</font> -->
                            <br><a href="https://www.nature.com/natmachintell/">Nature Machine Intelligence 2023</a> | <a href="https://www.nature.com/articles/s42256-021-00425-9">Paper</a> | <a href="https://github.com/funnyzhou/REFERS">Code</a> 
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2109.03201">
                            <b> Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Chixiang Lu*, Sibei Yang, Xiaoguang Han, Yizhou Yu.</font>
                            <!-- <br>[<a href="https://arxiv.org/abs/2109.04379" target="_blank">Paper</a>, <a href="https://github.com/Luchixiang/PCRL" target="_blank">Code</a>] -->
                            <!-- <br><font color="red">New state-of-the-arts of self-supervised medical image pre-training on both 2D and 3D data.</font> -->
                            <br> <a href="https://iccv2021.thecvf.com/home"> ICCV 2021 </a> | <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Preservational_Learning_Improves_Self-Supervised_Medical_Image_Models_by_Reconstructing_Diverse_ICCV_2021_paper.pdf">Paper</a> | <a href="https://github.com/Luchixiang/PCRL">Code</a>
                            <br>
                        </div>
                    </div>
                    <div class="publication media paperhi">
                        <div class="media-body ">
                            <a class="paper-title" href="https://arxiv.org/abs/2007.07423">
                            <b> Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations.</b></a>
                            <br> <font color="grey">Hong-Yu Zhou*, Shuang Yu*, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, Yefeng Zheng.</font>
                            <!-- <br>[<a href="https://arxiv.org/abs/2007.07423" target="_blank">Paper</a>, <a href="https://github.com/funnyzhou/C2L_MICCAI2020" target="_blank">Code</a>] -->
                            <!-- <br><font color="red">The first contrastive pre-training method in medical image analysis.</font> -->
                            <br> <a href="https://miccai2020.org/en/">MICCAI 2020</a> | <a href="https://arxiv.org/abs/2007.07423">Paper</a> | <a href="https://github.com/funnyzhou/C2L_MICCAI2020">Code</a>
                            <br>
                        </div>
                    </div> 
          </div>

    <div class="text_container visible">
        <div class="award-anchor" id="award-anchor"></div>
        <h2 name='award' id='award'>Selected Awards</h2>
        <div>
            <ul>
                <!-- <li><b>First place</b> in AASCE 2019, held in conjunction with MICCAI 2019. We proposed an approach to accurately estimate the spinal curvature in Adolescent Idiopathic Scoliosis (AIS). Press coverage: <a href="https://tech.china.com/article/20190929/kejiyuan0129377260.html">中华网</a> <a href="http://www.kejixun.com/article/190929/474694.shtml">科技讯</a> <a href=" http://yisheng.12120.net/news/jkpt/content_0910114529.html">中国好医生网</a> <a href="https://tech.ifeng.com/c/7qMGUlwISFH">凤凰网</a>.</li>
                <li><b>First place</b> in Segthor 2019 Challenge, held in conjunction with ISBI 2019. We proposed a method to segment organs at risk segmentation in Computed Tomography (CT) images. Press coverage: <a href="https://tech.qq.com/a/20190619/003614.htm">腾讯科技</a> <a href="https://www.jiqizhixin.com/articles/2019-06-19-8">机器之心</a> <a href="https://tech.huanqiu.com/article/9CaKrnKl1aA">环球网</a> <a href="https://www.qbitai.com/2019/06/3231.html">量子位</a> <a href="https://www.sohu.com/a/322090752_610300">搜狐网</a>.</li>
                <li><b>First place</b> in AI4Health Challenge, held by Thales group which provides solutions for over 50% of worldwide radiology examinations. In this challenge, we proposed a novel methodology to accurately detect key points in X-rays with a mean error smaller than <b>0.3 pixels</b>). Press coverage: <a href="http://www.ce.cn/xwzx/gnsz/gdxw/201907/12/t20190712_32605039.shtml">中国经济网</a> <a href="https://tech.huanqiu.com/article/9CaKrnKluBC">环球网</a> <a href="https://cloud.tencent.com/developer/article/1600187">腾讯新闻</a> <a href=""> <a href="http://www.geekpark.net/news/244645">极客公园</a> <a href="http://news.eastday.com/eastday/13news/auto/news/china/20190712/u7ai8688252.html">东方网</a>. </li> -->
                <li><b>First place</b> in AASCE 2019, held in conjunction with MICCAI 2019. We proposed an approach to accurately estimate the spinal curvature in Adolescent Idiopathic Scoliosis (AIS).</li>
                <li><b>First place</b> in Segthor 2019 Challenge, held in conjunction with ISBI 2019. We proposed a method to segment organs at risk segmentation in Computed Tomography (CT) images.</li>
                <li><b>First place</b> in AI4Health Challenge, held by Thales group which provides solutions for over 50% of worldwide radiology examinations. In this challenge, we proposed a novel methodology to accurately detect key points in X-rays with a mean error smaller than <b>0.3 pixels</b>.</li>
                <li><b>Second place</b> in ODIR 2019, held by Peking University.</li>
            </ul>
        </div>
    </div>


    <h2>Academic Service</h2>
    I regularly serve as reviewers for following conferences and journals.
    <ul>
	    <li>Conference: for ICLR, ICML, NeurIPS, CVPR, ICCV, ECCV, AAAI, MICCAI, etc.</li>
      <li>Journal: for Nature, Nature Biomedical Engineering, Nature Machine Intelligence, TPAMI, TMI, MedIA, TKDE, TMM, TIP, Pattern Recognition, iScience (Cell family), etc.</li>
    </ul>
</div>
</div>

<script>
    const element = document.getElementById("autoTyping");
    // const texts = ["Compound Medical AI Systems.", "Medical Representation Learning.", "Scalable Medical AI."]; // Array of texts
    const texts = [" AI systems that decipher individual health from varied scales."]
    let textIndex = 0; // Index to track which text is being typed
    let charIndex = 0; // Index to track the current character position
    let deleting = false;
    
    function updateText() {
      const currentText = texts[textIndex]; // Get the current text based on textIndex

      if (!deleting) {
        if (charIndex < currentText.length) {
          element.innerHTML += currentText.charAt(charIndex);
          charIndex++;
        } else {
          setTimeout(() => { deleting = true; }, 5000); // Wait a bit before starting to delete
        }
      } else {
        if (charIndex > 0) {
          element.innerHTML = element.innerHTML.slice(0, -1);
          charIndex--;
        } else {
          deleting = false; // Switch back to typing mode
          textIndex = (textIndex + 1) % texts.length; // Move to the next text and loop back to the first after the last
        }
      }
      setTimeout(updateText, deleting ? 50 : 150); // Speed up deletion
    }
    
    updateText(); // Start the effect
</script>

</body></html>
